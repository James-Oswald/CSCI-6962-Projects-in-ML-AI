{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "data = pd.read_csv('titanic.csv')\n",
    "data = data.sample(frac=1)\n",
    "x = data[[\"Age\", \"Pclass\", \"Fare\"]].to_numpy()\n",
    "y = data[\"Survived\"].to_numpy()\n",
    "datalen = len(x)\n",
    "split_percent = 0.05    #10% used for testing data \n",
    "split = math.floor(datalen*split_percent)\n",
    "\n",
    "#no need to shuffle, data already randomized in the .csv file\n",
    "x_test = x[:split].T\n",
    "x_train = x[split:].T\n",
    "y_test = y[:split]\n",
    "y_train = y[split:]\n",
    "\n",
    "#print(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainAcc: 0.67, TestAcc: 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/james/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#This is NOT my submission for task 3\n",
    "#This is just to double check my implementation's results against an offical implementation\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "m = LogisticRegression(penalty=\"none\", solver=\"sag\")\n",
    "m.fit(x_train.T, y_train)\n",
    "trainacc = m.score(x_train.T, y_train)\n",
    "testacc = m.score(x_test.T, y_test)\n",
    "print(f\"TrainAcc: {trainacc:0.2f}, TestAcc: {testacc:0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 99, Cost: 0.63, TrainAcc: 0.65, TestAcc: 0.75\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#hyperparams\n",
    "epochs = 100\n",
    "learning_rate = 0.003\n",
    "\n",
    "#init our weight vector based on how many features we have in the data\n",
    "weights = np.zeros(x.shape[1])\n",
    "bias = 0\n",
    "\n",
    "#compute the sigmoid of the input X\n",
    "#when X is an array, returns the array with sigmoid applied elementwise\n",
    "def sigmoid(X):\n",
    "    return 1/(1+np.exp(-X))\n",
    "\n",
    "# X: Model input\n",
    "def forward(X, weights, bias):\n",
    "    return sigmoid(np.dot(weights.T, X) + bias)\n",
    "\n",
    "#compute individual loss for each sample (the negitive has been factored out into the cost)\n",
    "#(in this case array of individual losses for each sample)\n",
    "# Y: Labels, A: Model Outputs\n",
    "def loss(Y, A):\n",
    "    return Y*np.log(A) + (1-Y)*np.log(1-A)\n",
    "\n",
    "#Compute cost\n",
    "# Y: Labels, A: Model Outputs\n",
    "def cost(Y, A):\n",
    "    return -1/len(A) * np.sum(loss(Y, A))\n",
    "\n",
    "#partial derivitive of cost with respect to weights\n",
    "def dCostWRTw(X, A, Y):\n",
    "    return np.dot(X, (A-Y).T)/len(A)\n",
    "\n",
    "#partial derivitive of cost with respect to bias\n",
    "def dCostWRTb(A, Y):\n",
    "    return np.sum(A-Y)/len(A)\n",
    "\n",
    "#Predict the \n",
    "# X: Model input\n",
    "def predict(X, weights, bias):\n",
    "    A = forward(X, weights, bias)\n",
    "    # Convert the entries of a into 0 (if activation <= 0.5) or\n",
    "    # 1 (if activation > 0.5) and store the predictions in a vector.\n",
    "    p = np.copy(A)\n",
    "    p[p <= 0.5] = 0\n",
    "    p[p > 0.5] = 1\n",
    "    return p \n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    return np.count_nonzero(predictions==labels)/len(predictions)\n",
    "\n",
    "#training loop\n",
    "for epoch in range(epochs):\n",
    "    A = forward(x_train, weights, bias)\n",
    "    c = cost(y_train, A)\n",
    "    \n",
    "    #compute grads of the cost func\n",
    "    dw = dCostWRTw(x_train, A, y_train)\n",
    "    db = dCostWRTb(A, y_train)\n",
    "\n",
    "    #update the weights + bias with respect to our grads scaled by our learning rate hyperparm\n",
    "    weights = weights - learning_rate * dw\n",
    "    bias = bias - learning_rate * db\n",
    "\n",
    "#Predict results and compute accuracy\n",
    "trainPredictions = predict(x_train, weights, bias)\n",
    "trainacc = accuracy(trainPredictions, y_train)\n",
    "testPredictions = predict(x_test, weights, bias)\n",
    "testacc = accuracy(testPredictions, y_test) \n",
    "print(f\"TrainAcc: {trainacc:0.2f}, TestAcc: {testacc:0.2f}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
